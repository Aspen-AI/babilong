{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## Run model and collect predictions on BABILong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('..')\n",
    "os.chdir('/home/booydar/rmt/babilong')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from babilong.prompts import DEFAULT_PROMPTS, DEFAULT_TEMPLATE, get_formatted_input\n",
    "from babilong.metrics import compare_answers, TASK_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'NousResearch/Meta-Llama-3-8B-Instruct'\n",
    "dtype = torch.bfloat16\n",
    "device = 'cuda:0'\n",
    "# device = 'cpu'\n",
    "max_length = 128000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True,\n",
    "                                             device_map=device, torch_dtype=dtype,\n",
    "                                             attn_implementation='flash_attention_2')\n",
    "model = model.eval()\n",
    "\n",
    "terminators = [\n",
    "                    tokenizer.eos_token_id,\n",
    "                    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\n",
    "    'num_beams': 1,\n",
    "    'do_sample': False,\n",
    "    'temperature': None,\n",
    "    'top_p': None,\n",
    "    'top_k': None,\n",
    "    'eos_token_id':terminators\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_examples(default_examples):\n",
    "    if len(default_examples) == 0:\n",
    "        return [], []\n",
    "    \n",
    "    examples = default_examples.split('<example>\\n')\n",
    "    examples = [e[:e.index(\"\\n</example>\")] for e in examples if len(e) > 0]\n",
    "    inputs = [e[:e.index(\"\\nAnswer\")] for e in examples]\n",
    "    outputs = [e[e.index(\"\\nAnswer\") + 9:] for e in examples]\n",
    "    return inputs, outputs\n",
    "\n",
    "def get_formatted_input(context, question, examples, instruction, post_prompt):\n",
    "    # pre_prompt - general instruction\n",
    "    # examples - in-context examples\n",
    "    # post_prompt - any additional instructions after examples\n",
    "    # context - text to use for qa\n",
    "    # question - question to answer based on context\n",
    "    inputs, outputs = format_examples(examples)\n",
    "    messages = []\n",
    "    if len(instruction) > 0:\n",
    "        messages.append({\"role\": \"system\", \"content\": instruction })\n",
    "\n",
    "    for i, o in zip(inputs, outputs):\n",
    "        messages += [\n",
    "            {\"role\": \"user\", \"content\": i},\n",
    "            {\"role\": \"assistant\", \"content\": o}\n",
    "        ]\n",
    "\n",
    "    messages.append( {\"role\": \"user\", \"content\": f\"{context} {question}\\n {post_prompt}\"} )\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on 1k examples on <=32k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['qa1', 'qa2','qa3', 'qa4', 'qa5']\n",
    "split_names = ['0k', '1k', '2k', '4k', '8k', '16k', '32k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot\n",
    "for task in tqdm(tasks, desc='tasks'):\n",
    "    prompt_cfg = {\n",
    "        'instruction': '',\n",
    "        'examples': '', \n",
    "        'post_prompt': '',\n",
    "        'template': None,\n",
    "    }\n",
    "    \n",
    "    prompt_name = [f'{k}_no' if len(prompt_cfg[k]) == 0 else f'{k}_yes' for k in prompt_cfg if k != 'template']\n",
    "    prompt_name = '_'.join(prompt_name)\n",
    "    for split_name in tqdm(split_names, desc='lengths'):\n",
    "        data = datasets.load_dataset(\"booydar/babilong-1k-samples\", split_name)\n",
    "        task_data = data[task]\n",
    "\n",
    "        outfile = Path(f'./babilong_evals_1k/{model_name}/{task}_{split_name}_{prompt_name}.csv')\n",
    "        outfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "        cfg_file = f'./babilong_evals_1k/{model_name}/{task}_{split_name}_{prompt_name}.json'\n",
    "        json.dump({'prompt': prompt_cfg, 'generate_kwargs': generate_kwargs}, open(cfg_file, 'w'), indent=4)\n",
    "\n",
    "        df = pd.DataFrame({'target': [], 'output': [], 'question': []})\n",
    "\n",
    "        for sample in tqdm(task_data):\n",
    "            target = sample['target']\n",
    "            context = sample['input']\n",
    "            question = sample['question']\n",
    "\n",
    "            messages = get_formatted_input(context, question, prompt_cfg['examples'],\n",
    "                                             prompt_cfg['instruction'], prompt_cfg['post_prompt'])\n",
    "            input_ids = tokenizer.apply_chat_template(messages,\n",
    "                                        add_generation_prompt=True,\n",
    "                                        return_tensors=\"pt\"\n",
    "                                        ).to(model.device)\n",
    "\n",
    "            sample_length = input_ids.shape[1]\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(input_ids=input_ids, max_length=sample_length+25, **generate_kwargs)\n",
    "            output = output[0][sample_length:]\n",
    "            output = tokenizer.decode(output, skip_special_tokens=True).strip()\n",
    "\n",
    "            df.loc[len(df)] = [target, output, question]\n",
    "            df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot\n",
    "\n",
    "for task in tqdm(tasks, desc='tasks'):\n",
    "    prompt_cfg = {\n",
    "        'instruction': DEFAULT_PROMPTS[task]['instruction'],\n",
    "        'examples': DEFAULT_PROMPTS[task]['examples'], \n",
    "        'post_prompt': DEFAULT_PROMPTS[task]['post_prompt'],\n",
    "        'template': None,\n",
    "    }\n",
    "    \n",
    "    prompt_name = [f'{k}_no' if len(prompt_cfg[k]) == 0 else f'{k}_yes' for k in prompt_cfg if k != 'template']\n",
    "    prompt_name = '_'.join(prompt_name)\n",
    "    for split_name in tqdm(split_names, desc='lengths'):\n",
    "        data = datasets.load_dataset(\"booydar/babilong-1k-samples\", split_name)\n",
    "        task_data = data[task]\n",
    "\n",
    "        outfile = Path(f'./babilong_evals_1k/{model_name}/{task}_{split_name}_{prompt_name}.csv')\n",
    "        outfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "        cfg_file = f'./babilong_evals_1k/{model_name}/{task}_{split_name}_{prompt_name}.json'\n",
    "        json.dump({'prompt': prompt_cfg, 'generate_kwargs': generate_kwargs}, open(cfg_file, 'w'), indent=4)\n",
    "\n",
    "        df = pd.DataFrame({'target': [], 'output': [], 'question': []})\n",
    "\n",
    "        for sample in tqdm(task_data):\n",
    "            target = sample['target']\n",
    "            context = sample['input']\n",
    "            question = sample['question']\n",
    "\n",
    "            messages = get_formatted_input(context, question, prompt_cfg['examples'],\n",
    "                                             prompt_cfg['instruction'], prompt_cfg['post_prompt'])\n",
    "            input_ids = tokenizer.apply_chat_template(messages,\n",
    "                                        add_generation_prompt=True,\n",
    "                                        return_tensors=\"pt\"\n",
    "                                        ).to(model.device)\n",
    "\n",
    "            sample_length = input_ids.shape[1]\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(input_ids=input_ids, max_length=sample_length+25, **generate_kwargs)\n",
    "            output = output[0][sample_length:]\n",
    "            output = tokenizer.decode(output, skip_special_tokens=True).strip()\n",
    "\n",
    "            df.loc[len(df)] = [target, output, question]\n",
    "            df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on 100 examples on >32k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['qa1', 'qa2','qa3', 'qa4', 'qa5']\n",
    "split_names = ['64k', '128k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot\n",
    "for task in tqdm(tasks, desc='tasks'):\n",
    "    prompt_cfg = {\n",
    "        'instruction': '',\n",
    "        'examples': '', \n",
    "        'post_prompt': '',\n",
    "        'template': None,\n",
    "    }\n",
    "    \n",
    "    prompt_name = [f'{k}_no' if len(prompt_cfg[k]) == 0 else f'{k}_yes' for k in prompt_cfg if k != 'template']\n",
    "    prompt_name = '_'.join(prompt_name)\n",
    "    for split_name in tqdm(split_names, desc='lengths'):\n",
    "        data = datasets.load_dataset(\"booydar/babilong\", split_name)\n",
    "        task_data = data[task]\n",
    "\n",
    "        outfile = Path(f'./babilong_evals_1k/{model_name}/{task}_{split_name}_{prompt_name}.csv')\n",
    "        outfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "        cfg_file = f'./babilong_evals_1k/{model_name}/{task}_{split_name}_{prompt_name}.json'\n",
    "        json.dump({'prompt': prompt_cfg, 'generate_kwargs': generate_kwargs}, open(cfg_file, 'w'), indent=4)\n",
    "\n",
    "        df = pd.DataFrame({'target': [], 'output': [], 'question': []})\n",
    "\n",
    "        for sample in tqdm(task_data):\n",
    "            target = sample['target']\n",
    "            context = sample['input']\n",
    "            question = sample['question']\n",
    "\n",
    "            messages = get_formatted_input(context, question, prompt_cfg['examples'],\n",
    "                                             prompt_cfg['instruction'], prompt_cfg['post_prompt'])\n",
    "            input_ids = tokenizer.apply_chat_template(messages,\n",
    "                                        add_generation_prompt=True,\n",
    "                                        return_tensors=\"pt\"\n",
    "                                        ).to(model.device)\n",
    "\n",
    "            sample_length = input_ids.shape[1]\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(input_ids=input_ids, max_length=sample_length+25, **generate_kwargs)\n",
    "            output = output[0][sample_length:]\n",
    "            output = tokenizer.decode(output, skip_special_tokens=True).strip()\n",
    "\n",
    "            df.loc[len(df)] = [target, output, question]\n",
    "            df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot\n",
    "\n",
    "for task in tqdm(tasks, desc='tasks'):\n",
    "    prompt_cfg = {\n",
    "        'instruction': DEFAULT_PROMPTS[task]['instruction'],\n",
    "        'examples': DEFAULT_PROMPTS[task]['examples'], \n",
    "        'post_prompt': DEFAULT_PROMPTS[task]['post_prompt'],\n",
    "        'template': None,\n",
    "    }\n",
    "    \n",
    "    prompt_name = [f'{k}_no' if len(prompt_cfg[k]) == 0 else f'{k}_yes' for k in prompt_cfg if k != 'template']\n",
    "    prompt_name = '_'.join(prompt_name)\n",
    "    for split_name in tqdm(split_names, desc='lengths'):\n",
    "        data = datasets.load_dataset(\"booydar/babilong\", split_name)\n",
    "        task_data = data[task]\n",
    "\n",
    "        outfile = Path(f'./babilong_evals_1k/{model_name}/{task}_{split_name}_{prompt_name}.csv')\n",
    "        outfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "        cfg_file = f'./babilong_evals_1k/{model_name}/{task}_{split_name}_{prompt_name}.json'\n",
    "        json.dump({'prompt': prompt_cfg, 'generate_kwargs': generate_kwargs}, open(cfg_file, 'w'), indent=4)\n",
    "\n",
    "        df = pd.DataFrame({'target': [], 'output': [], 'question': []})\n",
    "\n",
    "        for sample in tqdm(task_data):\n",
    "            target = sample['target']\n",
    "            context = sample['input']\n",
    "            question = sample['question']\n",
    "\n",
    "            messages = get_formatted_input(context, question, prompt_cfg['examples'],\n",
    "                                             prompt_cfg['instruction'], prompt_cfg['post_prompt'])\n",
    "            input_ids = tokenizer.apply_chat_template(messages,\n",
    "                                        add_generation_prompt=True,\n",
    "                                        return_tensors=\"pt\"\n",
    "                                        ).to(model.device)\n",
    "\n",
    "            sample_length = input_ids.shape[1]\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(input_ids=input_ids, max_length=sample_length+25, **generate_kwargs)\n",
    "            output = output[0][sample_length:]\n",
    "            output = tokenizer.decode(output, skip_special_tokens=True).strip()\n",
    "\n",
    "            df.loc[len(df)] = [target, output, question]\n",
    "            df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = './babilong_evals_1k'\n",
    "model_names = ['NousResearch/Meta-Llama-3-8B-Instruct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['qa1', 'qa2', 'qa3', 'qa4', 'qa5']\n",
    "lengths = ['0k', '1k', '2k', '4k', '8k', '16k', '32k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model_name in model_names:\n",
    "    prompt_name = 'instruction_no_examples_no_post_prompt_no'\n",
    "    accuracy = np.ones((len(tasks), len(lengths))) * -1\n",
    "    try:\n",
    "        for j, task in enumerate(tasks):\n",
    "            for i, ctx_length in enumerate(lengths):\n",
    "                fname = f'{results_folder}/{model_name}/{task}_{ctx_length}_{prompt_name}.csv'\n",
    "                if not os.path.isfile(fname):\n",
    "                    # print(f'No such file: {fname}')\n",
    "                    continue\n",
    "                \n",
    "                df = pd.read_csv(fname)\n",
    "                \n",
    "                if df['output'].dtype != object:\n",
    "                    df['output'] = df['output'].astype(str)\n",
    "                df['output'] = df['output'].fillna('')\n",
    "\n",
    "\n",
    "                df['correct'] = df.apply(lambda row: compare_answers(target=row['target'], output=row['output'], question=row['question'],\n",
    "                                                                     task_labels=TASK_LABELS[task]), axis=1)\n",
    "                score = df['correct'].sum()\n",
    "                accuracy[j, i] = 100 * score / len(df) if len(df) > 0 else 0\n",
    "\n",
    "        prompt_name = 'instruction_yes_examples_yes_post_prompt_yes'\n",
    "        accuracy_fs = np.ones((len(tasks), len(lengths))) * -1\n",
    "        for j, task in enumerate(tasks):\n",
    "            for i, ctx_length in enumerate(lengths):\n",
    "                fname = f'{results_folder}/{model_name}/{task}_{ctx_length}_{prompt_name}.csv'\n",
    "                if not os.path.isfile(fname):\n",
    "                    # print(f'No such file: {fname}')\n",
    "                    continue\n",
    "                \n",
    "                df = pd.read_csv(fname)\n",
    "                \n",
    "                if df['output'].dtype != object:\n",
    "                    df['output'] = df['output'].astype(str)\n",
    "                df['output'] = df['output'].fillna('')\n",
    "\n",
    "\n",
    "                df['correct'] = df.apply(lambda row: compare_answers(target=row['target'], output=row['output'], question=row['question'],\n",
    "                                                                     task_labels=TASK_LABELS[task]), axis=1)\n",
    "                score = df['correct'].sum()\n",
    "                accuracy_fs[j, i] = 100 * score / len(df) if len(df) > 0 else 0\n",
    "\n",
    "        results[model_name] = {'zero': accuracy, 'few': accuracy_fs}\n",
    "        \n",
    "\n",
    "        # Set large font sizes for better visibility in the PDF\n",
    "        matplotlib.rc('font', size=14)\n",
    "\n",
    "        # Create a colormap for the heatmap\n",
    "        cmap = LinearSegmentedColormap.from_list('ryg', [\"red\", \"yellow\", \"green\"], N=256)\n",
    "\n",
    "        # Create the heatmap\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 3.5))  # Adjust the size as necessary\n",
    "        \n",
    "        sns.heatmap(accuracy, cmap=cmap, vmin=0, vmax=100, annot=True, fmt=\".0f\",\n",
    "                    linewidths=.5, xticklabels=lengths, yticklabels=tasks, ax=ax[0])\n",
    "        \n",
    "        sns.heatmap(accuracy_fs, cmap=cmap, vmin=0, vmax=100, annot=True, fmt=\".0f\",\n",
    "                    linewidths=.5, xticklabels=lengths, yticklabels=tasks, ax=ax[1])\n",
    "        \n",
    "        ax[0].set_title(f'Zero shot \\n {model_name}')\n",
    "        ax[1].set_title(f'Few shot \\n {model_name}')\n",
    "        ax[0].set_xlabel('Context size')\n",
    "        ax[1].set_xlabel('Context size')\n",
    "        ax[0].set_ylabel('Tasks')\n",
    "        ax[1].set_ylabel('Tasks')\n",
    "\n",
    "        # Save the figure to a PDF\n",
    "        # plt.savefig('all_tasks_performance.pdf', bbox_inches='tight')\n",
    "        plt.show()\n",
    "    except KeyError as e:\n",
    "        print(f\"question not found in {model_name}\")\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print  table for copying to google sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0k</th>\n",
       "      <th>1k</th>\n",
       "      <th>2k</th>\n",
       "      <th>4k</th>\n",
       "      <th>8k</th>\n",
       "      <th>16k</th>\n",
       "      <th>32k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>qa1</th>\n",
       "      <td>98</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qa2</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qa3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qa4</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qa5</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0k  1k  2k  4k  8k  16k  32k\n",
       "qa1  98  -1  -1  -1  -1   -1   -1\n",
       "qa2  -1  -1  -1  -1  -1   -1   -1\n",
       "qa3  -1  -1  -1  -1  -1   -1   -1\n",
       "qa4  -1  -1  -1  -1  -1   -1   -1\n",
       "qa5  -1  -1  -1  -1  -1   -1   -1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn = model_name\n",
    "\n",
    "tab = results[mn]['zero']\n",
    "tab = pd.DataFrame(tab, index=tasks, columns=lengths[:tab.shape[1]])\n",
    "\n",
    "tab_few = results[mn]['few']\n",
    "tab_few = pd.DataFrame(tab_few, index=tasks, columns=lengths[:tab_few.shape[1]])\n",
    "\n",
    "tab['avg'] = tab.mean(axis=1)\n",
    "tab_few['avg'] = tab_few.mean(axis=1)\n",
    "\n",
    "best_tab = tab_few.copy()\n",
    "for i, row in tab.iterrows():\n",
    "    if row.avg > best_tab.loc[i].avg:\n",
    "        best_tab.loc[i] = row\n",
    "\n",
    "best_tab.round().astype(int)[best_tab.columns[:-1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
